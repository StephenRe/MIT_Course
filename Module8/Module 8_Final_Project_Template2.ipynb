{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sre\\AppData\\Local\\Temp\\ipykernel_17716\\3972124454.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  total = tips.groupby('day')['total_bill'].sum().reset_index()\n",
      "C:\\Users\\sre\\AppData\\Local\\Temp\\ipykernel_17716\\3972124454.py:15: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  smoker = tips[tips.smoker=='Yes'].groupby('day')['total_bill'].sum().reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>total_bill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thur</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fri</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sat</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sun</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day  total_bill\n",
       "0  Thur       100.0\n",
       "1   Fri       100.0\n",
       "2   Sat       100.0\n",
       "3   Sun       100.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# load dataset\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "# tips.head()\n",
    "# # set the figure size\n",
    "# plt.figure(figsize=(14, 14))\n",
    "\n",
    "# # from raw value to percentage\n",
    "total = tips.groupby('day')['total_bill'].sum().reset_index()\n",
    "smoker = tips[tips.smoker=='Yes'].groupby('day')['total_bill'].sum().reset_index()\n",
    "smoker\n",
    "smoker['total_bill'] = [i / j * 100 for i,j in zip(smoker['total_bill'], total['total_bill'])]\n",
    "smoker\n",
    "total['total_bill'] = [i / j * 100 for i,j in zip(total['total_bill'], total['total_bill'])]\n",
    "total\n",
    "\n",
    "# # bar chart 1 -> top bars (group of 'smoker=No')\n",
    "# bar1 = sns.barplot(x=\"day\",  y=\"total_bill\", data=total, color='darkblue')\n",
    "\n",
    "# # bar chart 2 -> bottom bars (group of 'smoker=Yes')\n",
    "# bar2 = sns.barplot(x=\"day\", y=\"total_bill\", data=smoker, color='lightblue')\n",
    "\n",
    "# # add legend\n",
    "# top_bar = mpatches.Patch(color='darkblue', label='smoker = No')\n",
    "# bottom_bar = mpatches.Patch(color='lightblue', label='smoker = Yes')\n",
    "# plt.legend(handles=[top_bar, bottom_bar])\n",
    "\n",
    "# # show the graph\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final project for this module, you are asked to use ETL together with the skills you have learned about Python and MySQL in the previous modules to understand spending patterns.\n",
    "\n",
    "This module's project is divided into two main parts: Extract-Transform-Load (ETL) and Analysis and Visualization.\n",
    "\n",
    "Your challenge in this project is to implement the steps suggested by Dr. Sanchez in his videos throughout the module and prove that you have a a clear understanding of each of them by being able to describe and justify them. You will also be tested on your ability to conduct your own analysis to understand spending patterns.\n",
    "\n",
    "Before you fill out the project outline template below, make sure you:\n",
    "\n",
    "- Read through the template completely to understand the instructions for the structure of the project.\n",
    "- Have a clear understanding of what to do to create a model that will return the results you want to find.\n",
    "- Use Markdown to edit the template.\n",
    "- Include any screenshots of your code (both Python and MySQL) and of your program windows (Excel, Terminal, VS Code, MySQL Workbench) to demonstrate your steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The purpose of this Jupyter Notebook is to give you a structure to follow when you are solving your problem and developing your model with Python. Make sure you follow it carefully. You can add more subsections if needed, but remember to fill out every section provided in the template.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Delete all cells above, including this one, before submitting your final Notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "**Your_Name**\n",
    "\n",
    "Add the title of your project and delete the default one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- [Abstract](#Abstract)\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. Extract-Transform-Load](#2.-Extract-Transform-Load)\n",
    "    - [2.1 The ETL Process](#2.1-The-ETL-Process)\n",
    "    - [2.2 Data Exploration](#2.2-Data-Exploration)\n",
    "    - [2.3 Data Preparation](#2.3-Data-Preparation)\n",
    "    - [2.4 Read the Data Using Python](#2.4-Reading-the-Data-Using-Python)\n",
    "         - [2.4.1 Reading Sample Data](#2.4.1-Reading-Sample-Data)\n",
    "         - [2.4.2 Reading the MRST Data](#2.4.2-Reading-the-MRST-Data)\n",
    "    - [2.5 Writing an Installation Script](#2.5-Writing-an-Installation-Script)\n",
    "- [3. Analysis and Visualization](#3.-Project-Description)\n",
    "    - [3.1 Running Queries in MySQL Workbech](#3.1-Running-Queries-in-MySQL-Workbech)\n",
    "    - [3.2 Running Queries From Python](#3.2-Running-Queries-From-Python)\n",
    "    - [3.3 Explore Trends](#3.3-Explore-Trends)\n",
    "    - [3.4 Explore Percentage Change](#3.4-Explore-Percentage-Change)\n",
    "    - [3.5 Explore Rolling Time Windows](#3.5-Explore-Rolling-Time-Windows)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "\n",
    "##  Abstract\n",
    "\n",
    "This is a brief description (150 words or less) of your analysis and the results of your model. Complete this portion of the template after you are done working on your project.\n",
    "\n",
    "Included all four aspects of the final project and clearly and succinctly summarized the: \n",
    "- Purpose of the project \n",
    "- Models or methods used in the project \n",
    "- Major findings from the analysis \n",
    "- Interpretations and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Introduce your project using 300 words or less. Describe all the processes you followed to create your ETL, Analysis, and Visualization project. Start by summarizing the steps that you intend to perform and then elaborate on this section after you have completed your project.\n",
    "\n",
    "Introduced the project and provided a detailed description of the required sections: \n",
    "- An overview of the problem-solving framework \n",
    "- Workflow model \n",
    "- Processes used to solve the identified data engineering problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "## 2. Extract-Transform-Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.1 The ETL Process\n",
    "\n",
    "Key Steps for processing MRTS dataset:\n",
    "- read each tab in the .xls file into individual csv files.\n",
    "- clean data.  See section \"2.3 Data Preparation\" for details.\n",
    "- create installation script for MySQL\n",
    "- connect to MySQL\n",
    "- run installation script\n",
    "\n",
    "\n",
    "\n",
    "Described the key steps to perform ETL on a general dataset in detail by including personal insights.\n",
    "\n",
    "Describe, using your own words, the key steps to perform ETL on the provided MRTS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.2 Data Exploration\n",
    "\n",
    "#### !!!Described the dataset accurately and demonstrated that independent research was conducted to familiarize with the data.\n",
    "\n",
    "Did a lot of exploration of the NAICS dataset, and trends presented on St. Louis Fed and the Bureau of Labor Statistics websites.  The NAICS dataset for MRTS is only concerned with NAICS sectors 44 and 45 (Retail Trade), and subsector 722 (Food Services and Drinking Places).\n",
    "\n",
    "I went to the St Louis Fed website to get the seasonaly adjusted monthly CPI index so i could adjust the amounts into Feb 2021 dollars to show actual vs inflation adjusted trends.\n",
    "\n",
    "I found it interesting, but not surprising, that there were choices made in 1992 with the first publication that have followed through to today.  For instance, including 4411 and 4412 on the same line, but not breaking them out on subsequent lines.  I assume they continued those choices all the way to today for consisitency, but i would be curious to know why those decisions were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.3 Data Preparation\n",
    "\n",
    "#### !!!Described and justified precisely which modifications were made to the dataset and included one or more modifications not demonstrated in the video.\n",
    "\n",
    "My goal was to put all of the csv files into one table with the grain of month_year and category.  \n",
    "the columns would be: \n",
    "- NAICS_code_class: the top level description of each code length.\n",
    "- Business Type: NAICS category name\n",
    "- month_year: the month and year for the sample. \n",
    "\n",
    "- year\n",
    "- month\n",
    "\n",
    "- amount: dollar amount of sample\n",
    "- adjusted_amount: this is CPI inflation (all minus food and energy) adjusted and seasonaly adjusted.\n",
    "\n",
    "In that format, it will be easier to do some time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.4 Read the Data Using Python\n",
    "\n",
    "The Pandas library is great for exactly this sort of thing. Once your data is read into a dataframe, pandas can be used to do a lot, if not all of your data cleaning tasks.  I am not normally used to data cleaning with anything by SQL, so i have been impressed with the Pandas versitiliy.  Below you will see the code i used to clean each of the individual yearly csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.4.1 Reading Sample Data\n",
    "\n",
    "I have used the 2018 data in csv form.  No particular reason.  It was the first complete year that i started working with and just continued with it because it seemed like a good testbed.\n",
    "\n",
    "I also used the CPI data described below to enhance the data.  I downloaded it from the site and created the monthly ratios using the formula:  \n",
    "        **(feb 2021 index / each monthly index) *  amount**    \n",
    "The first term gets us the ratio of month to feb 2021.  Then multiplying the amounts of each category in the MRTS sheets gets us the adjusted amounts.\n",
    "\n",
    "I then used the code for the 2018 cleaning research to create the overall cleaning script in 2.4.2 below.  I have fully commented the code so you can follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.4.2 Reading the MRTS Data\n",
    "\n",
    "Step by Step description of the cleaning script below.  The code is fully commented also.\n",
    "- break up xls file into individual yearly csv files\n",
    "- for each yearly csv file:\n",
    "    - load file\n",
    "    - update column names\n",
    "    - add surogate keys for first 7 lines\n",
    "    - keep rows of unadjusted data and drop the rest\n",
    "    - cast columns as numeric\n",
    "    - disaggregate '4411,4412' row and add new 4412 row\n",
    "    - drop rows '4411,4412' and '442,443' as they are unneeded\n",
    "    - adjusted NAICS code for row '722513, 722514, 722515'\n",
    "    - fix '(NA)' and '(S)' cells as NaN\n",
    "    - reset index\n",
    "    - add df to dict\n",
    "    - use pd.melt() to unpivot the data into the grain:\n",
    "        - NAICS_code\n",
    "        - month_year\n",
    "- concat the year dfs into one full_df\n",
    "- add columns:\n",
    "    - year\n",
    "    - month\n",
    "    - month_start (used this one to merge with inflation adjustment df)\n",
    "- load inflation adjustment csv into df\n",
    "- merge inflation adjustment df into full_df\n",
    "- add inflation adjusted dollar amount column\n",
    "- create final_df by selecting only needed columns from merged df\n",
    "- write final_df to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2021 to MRTS/MRTS_year_files/MRTS_2021.csv\n",
      "Saved 2020 to MRTS/MRTS_year_files/MRTS_2020.csv\n",
      "Saved 2019 to MRTS/MRTS_year_files/MRTS_2019.csv\n",
      "Saved 2018 to MRTS/MRTS_year_files/MRTS_2018.csv\n",
      "Saved 2017 to MRTS/MRTS_year_files/MRTS_2017.csv\n",
      "Saved 2016 to MRTS/MRTS_year_files/MRTS_2016.csv\n",
      "Saved 2015 to MRTS/MRTS_year_files/MRTS_2015.csv\n",
      "Saved 2014 to MRTS/MRTS_year_files/MRTS_2014.csv\n",
      "Saved 2013 to MRTS/MRTS_year_files/MRTS_2013.csv\n",
      "Saved 2012 to MRTS/MRTS_year_files/MRTS_2012.csv\n",
      "Saved 2011 to MRTS/MRTS_year_files/MRTS_2011.csv\n",
      "Saved 2010 to MRTS/MRTS_year_files/MRTS_2010.csv\n",
      "Saved 2009 to MRTS/MRTS_year_files/MRTS_2009.csv\n",
      "Saved 2008 to MRTS/MRTS_year_files/MRTS_2008.csv\n",
      "Saved 2007 to MRTS/MRTS_year_files/MRTS_2007.csv\n",
      "Saved 2006 to MRTS/MRTS_year_files/MRTS_2006.csv\n",
      "Saved 2005 to MRTS/MRTS_year_files/MRTS_2005.csv\n",
      "Saved 2004 to MRTS/MRTS_year_files/MRTS_2004.csv\n",
      "Saved 2003 to MRTS/MRTS_year_files/MRTS_2003.csv\n",
      "Saved 2002 to MRTS/MRTS_year_files/MRTS_2002.csv\n",
      "Saved 2001 to MRTS/MRTS_year_files/MRTS_2001.csv\n",
      "Saved 2000 to MRTS/MRTS_year_files/MRTS_2000.csv\n",
      "Saved 1999 to MRTS/MRTS_year_files/MRTS_1999.csv\n",
      "Saved 1998 to MRTS/MRTS_year_files/MRTS_1998.csv\n",
      "Saved 1997 to MRTS/MRTS_year_files/MRTS_1997.csv\n",
      "Saved 1996 to MRTS/MRTS_year_files/MRTS_1996.csv\n",
      "Saved 1995 to MRTS/MRTS_year_files/MRTS_1995.csv\n",
      "Saved 1994 to MRTS/MRTS_year_files/MRTS_1994.csv\n",
      "Saved 1993 to MRTS/MRTS_year_files/MRTS_1993.csv\n",
      "Saved 1992 to MRTS/MRTS_year_files/MRTS_1992.csv\n"
     ]
    }
   ],
   "source": [
    "# First, we need to loop through the tabs in the .xls sheet and save them as seperate csv files.\n",
    "xls_file = 'MRTS/mrtssales92-present.xls'\n",
    "\n",
    "xls = pd.ExcelFile(xls_file, engine='xlrd')\n",
    "\n",
    "for sheet_name in xls.sheet_names:\n",
    "    df = xls.parse(sheet_name)\n",
    "    csv_file = f\"MRTS/MRTS_year_files/MRTS_{sheet_name}.csv\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    print(f\"Saved {sheet_name} to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index`,`NAICS_code`,`Business_type`,`January_2018`,`February_2018`,`March_2018`,`April_2018`,`May_2018`,`June_2018`,`July_2018`,`August_2018`,`September_2018`,`October_2018`,`November_2018`,`December_2018`,`TOTAL_2018'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of basic column names to be customized later\n",
    "col_names=['NAICS_code', 'Business_type', 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December', 'TOTAL', 'Feb. 2021(p)']\n",
    "\n",
    "# list of column names that are edge cases that need to be removed\n",
    "removed_col_names = ['CY CUM', 'PY CUM', 'Feb. 2021(p)_2016', 'Unnamed: 15']\n",
    "\n",
    "file = 'MRTS_2018.csv'\n",
    "year = file[5:9]\n",
    "df_col_names = []\n",
    "\n",
    "new_df = pd.read_csv(f'MRTS/MRTS_year_files/{file}', skiprows=[0,1,2,3,5] ,header=0)#, names=['NAICS_code', 'Business_type'] + df_col_names)\n",
    "n_col = 0\n",
    "for n in new_df.columns:\n",
    "    if n not in removed_col_names:\n",
    "        n_col+=1\n",
    "for n in col_names[2:n_col]:\n",
    "        df_col_names.append(f'{n}_{year}')\n",
    "new_df = new_df.iloc[:,:n_col]\n",
    "        \n",
    "new_col_names = ['NAICS_code', 'Business_type'] + df_col_names\n",
    "new_df.columns = new_col_names\n",
    "\n",
    "# add \"sector\" keys to the 7 aggregate values at the top of the sheet.\n",
    "# I am keeping them because it might be easier to do time series analysis using preaggregated than building the aggregations myself.\n",
    "new_df.iloc[0:7,0] = range(1,8)\n",
    "\n",
    "#removing the bottom adjusted data and all of the rest of the unneccesary rows.\n",
    "new_df = new_df[:65]\n",
    "\n",
    "# I need to reduce the last code in the series to only one NAICS code.\n",
    "# There is no way to break out or determine which code to use.\n",
    "# After doing some research, it makes sense to use 722513 as the stand-in for these codes.\n",
    "new_df.iloc[-1,0] = 722513\n",
    "\n",
    "# replacing '(S)' values with 0 for now so i can convert all of the columns to int data type.\n",
    "# I will get rid of those cells when i put everything together.\n",
    "new_df[new_df.iloc[:,2:] == '(S)'] = np.nan\n",
    "new_df[new_df.iloc[:,2:] == '(NA)'] = np.nan\n",
    "\n",
    "# converting columns to numeric\n",
    "for col in new_df.columns[2:]:\n",
    "    new_df[col] = pd.to_numeric(new_df[col])\n",
    "    \n",
    "# codes 4411 and 4412 share a line.  4411 is browken out individually, but not 4412.\n",
    "# I decided to create a row that subtracts 4411 from the total to get 4412 by itself.\n",
    "new_row = {}\n",
    "new_row['NAICS_code']=4412\n",
    "new_row['Business_type']='Other Motor Vehicle Dealers'\n",
    "for col in new_df.columns[2:]:\n",
    "    new_row[col] = new_df.iloc[8][col]-new_df.iloc[9][col]\n",
    "new_df.loc[len(new_df)] = new_row\n",
    "\n",
    "# now that i have the disggregated row for 4412, i can get rid of the preaggregated row.\n",
    "# dropping NAICS_code=='4411,4412' because, now that we have split it out, we dont need this line.\n",
    "new_df.drop(new_df[new_df.NAICS_code=='4411,4412'].index, inplace=True)\n",
    "\n",
    "# dropping NAICS_code=='442,443' because both subcatagories have already been broken out below, so this line isnt needed.\n",
    "new_df.drop(new_df[new_df.NAICS_code=='442,443'].index, inplace=True)\n",
    "\n",
    "# reset index\n",
    "new_df.reset_index(inplace=True)\n",
    "new_df.drop(columns=['index'])\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to your output text file\n",
    "output_file_path = 'MRTS/MRTS_year_files/naics_data_insert_sql.txt'\n",
    "\n",
    "# Generate the column names for the SQL statement\n",
    "insert_cols = \",\".join([str(i) for i in df.columns.tolist()])\n",
    "\n",
    "# # Open the output text file in write mode\n",
    "# with open(output_file_path, 'w') as file:\n",
    "#     # Iterate through each row in the DataFrame\n",
    "#     for i, row in df.iterrows():\n",
    "#         # Create the SQL INSERT statement\n",
    "#         sql = \"INSERT INTO your_table_name (\" + insert_cols + \") VALUES (\" + \"%s,\"*(len(row)-1) + \"%s);\" % tuple(row)\n",
    "#         # Write the SQL statement to the text file\n",
    "#         file.write(sql + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"January_1992\" at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mlib.pyx:2391\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"January_1992\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# converting columns to numeric\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m new_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m2\u001b[39m:]:\n\u001b[1;32m---> 61\u001b[0m     new_df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# codes 4411 and 4412 share a line.  4411 is browken out individually, but not 4412.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# I decided to create a row that subtracts 4411 from the total to get 4412 by itself.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m new_row \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\sre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\tools\\numeric.py:232\u001b[0m, in \u001b[0;36mto_numeric\u001b[1;34m(arg, errors, downcast, dtype_backend)\u001b[0m\n\u001b[0;32m    230\u001b[0m coerce_numeric \u001b[38;5;241m=\u001b[39m errors \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 232\u001b[0m     values, new_mask \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_numeric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_to_masked_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStringDtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues_dtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow_numpy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mlib.pyx:2433\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"January_1992\" at position 0"
     ]
    }
   ],
   "source": [
    "# Full cleaning script\n",
    "\n",
    "# create dict to hold all dataframes\n",
    "df_dict = {}\n",
    "\n",
    "# list of basic column names to be customized later\n",
    "col_names=['NAICS_code', 'Business_type', 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December', 'TOTAL', 'Feb. 2021(p)']\n",
    "\n",
    "# list of column names that are edge cases that need to be removed\n",
    "removed_col_names = ['CY CUM', 'PY CUM', 'Feb. 2021(p)_2016', 'Unnamed: 15']\n",
    "\n",
    "'''\n",
    "main cleaning script:\n",
    "for each yearly csv file:\n",
    "    load file\n",
    "    update column names\n",
    "    add surogate keys for first 7 lines\n",
    "    keep rows of unadjusted data and drop the rest\n",
    "    cast columns as numeric\n",
    "    disaggregate 4411,4412 row and add new 4412 row\n",
    "    drop unneeded rows\n",
    "    fix '(NA)' and '(S)' cells as NaN\n",
    "    reset index\n",
    "    add df to dict\n",
    "'''\n",
    "for file in os.listdir('MRTS/MRTS_year_files'):\n",
    "    year = file[5:9]\n",
    "    df_col_names = []\n",
    "\n",
    "    new_df = pd.read_csv(f'MRTS/MRTS_year_files/{file}', skiprows=[0,1,2,3,5] ,header=0)#, names=['NAICS_code', 'Business_type'] + df_col_names)\n",
    "    n_col = 0\n",
    "    for n in new_df.columns:\n",
    "        if n not in removed_col_names:\n",
    "            n_col+=1\n",
    "    for n in col_names[2:n_col]:\n",
    "         df_col_names.append(f'{n}_{year}')\n",
    "    new_df = new_df.iloc[:,:n_col]\n",
    "         \n",
    "    new_col_names = ['NAICS_code', 'Business_type'] + df_col_names\n",
    "    new_df.columns = new_col_names\n",
    "\n",
    "    # add \"sector\" keys to the 7 aggregate values at the top of the sheet.\n",
    "    # I am keeping them because it might be easier to do time series analysis using preaggregated than building the aggregations myself.\n",
    "    new_df.iloc[0:7,0] = range(1,8)\n",
    "\n",
    "    #removing the bottom adjusted data and all of the rest of the unneccesary rows.\n",
    "    new_df = new_df[:65]\n",
    "\n",
    "    # I need to reduce the last code in the series to only one NAICS code.\n",
    "    # There is no way to break out or determine which code to use.\n",
    "    # After doing some research, it makes sense to use 722513 as the stand-in for these codes.\n",
    "    new_df.iloc[-1,0] = 722513\n",
    "\n",
    "    # replacing '(S)' values with 0 for now so i can convert all of the columns to int data type.\n",
    "    # I will get rid of those cells when i put everything together.\n",
    "    new_df[new_df.iloc[:,2:] == '(S)'] = np.nan\n",
    "    new_df[new_df.iloc[:,2:] == '(NA)'] = np.nan\n",
    "\n",
    "    # converting columns to numeric\n",
    "    for col in new_df.columns[2:]:\n",
    "        new_df[col] = pd.to_numeric(new_df[col])\n",
    "        \n",
    "    # codes 4411 and 4412 share a line.  4411 is browken out individually, but not 4412.\n",
    "    # I decided to create a row that subtracts 4411 from the total to get 4412 by itself.\n",
    "    new_row = {}\n",
    "    new_row['NAICS_code']=4412\n",
    "    new_row['Business_type']='Other Motor Vehicle Dealers'\n",
    "    for col in new_df.columns[2:]:\n",
    "        new_row[col] = new_df.iloc[8][col]-new_df.iloc[9][col]\n",
    "    new_df.loc[len(new_df)] = new_row\n",
    "\n",
    "    # now that i have the disggregated row for 4412, i can get rid of the preaggregated row.\n",
    "    # dropping NAICS_code=='4411,4412' because, now that we have split it out, we dont need this line.\n",
    "    new_df.drop(new_df[new_df.NAICS_code=='4411,4412'].index, inplace=True)\n",
    "\n",
    "    # dropping NAICS_code=='442,443' because both subcatagories have already been broken out below, so this line isnt needed.\n",
    "    new_df.drop(new_df[new_df.NAICS_code=='442,443'].index, inplace=True)\n",
    "\n",
    "    # reset index\n",
    "    new_df.reset_index(inplace=True)\n",
    "    new_df.head(15)\n",
    "\n",
    "    df_dict[f'df_{year}'] = new_df\n",
    "    # print(df_dict[f'df_{year}'].columns)\n",
    "\n",
    "# unpivoting eaech file into NAICS_code, month_year grain\n",
    "# list for conating dfs\n",
    "melted_list = []\n",
    "\n",
    "for file in os.listdir('MRTS/MRTS_year_files'):\n",
    "    # create list of value columns to unpivot\n",
    "    year = file[5:9]\n",
    "    a = df_dict[f'df_{year}'].columns[3:]\n",
    "\n",
    "    # melt (unpivot) each file\n",
    "    df_dict[f'df_{year}_melted'] = pd.melt(df_dict[f'df_{year}'],\n",
    "                                          id_vars=['NAICS_code', 'Business_type'], \n",
    "                                          value_vars=a,\n",
    "                                          var_name='month_year', \n",
    "                                          value_name='Raw_Amount')\n",
    "    # building final df\n",
    "    melted_list.append(df_dict[f'df_{year}_melted'])\n",
    "    mrts_df = pd.concat(melted_list, ignore_index=True)\n",
    "# adding month and year columns for easier analysis\n",
    "mrts_df['year'] = mrts_df['month_year'].str[-4:]\n",
    "mrts_df['month'] = mrts_df['month_year'].str[:-5]\n",
    "\n",
    "# add month_start date column to join on cpi_df\n",
    "month_dict = dict(zip(pd.date_range('1992-01-01', freq='ME', periods=12).strftime('%B'),\n",
    "             pd.date_range('1992-01-01', freq='ME', periods=12).strftime('%m') + '-01'))\n",
    "mrts_df['month_start'] = mrts_df['year'] + '-' + final_df['month'].map(month_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mrts_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmrts_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mrts_df' is not defined"
     ]
    }
   ],
   "source": [
    "mrts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>CPILFESL</th>\n",
       "      <th>feb2021_index</th>\n",
       "      <th>feb2021_multiple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>145.1</td>\n",
       "      <td>270.813</td>\n",
       "      <td>1.866389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992-02-01</td>\n",
       "      <td>145.4</td>\n",
       "      <td>270.813</td>\n",
       "      <td>1.862538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992-03-01</td>\n",
       "      <td>145.9</td>\n",
       "      <td>270.813</td>\n",
       "      <td>1.856155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992-04-01</td>\n",
       "      <td>146.3</td>\n",
       "      <td>270.813</td>\n",
       "      <td>1.851080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1992-05-01</td>\n",
       "      <td>146.8</td>\n",
       "      <td>270.813</td>\n",
       "      <td>1.844775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DATE  CPILFESL  feb2021_index  feb2021_multiple\n",
       "0  1992-01-01     145.1        270.813          1.866389\n",
       "1  1992-02-01     145.4        270.813          1.862538\n",
       "2  1992-03-01     145.9        270.813          1.856155\n",
       "3  1992-04-01     146.3        270.813          1.851080\n",
       "4  1992-05-01     146.8        270.813          1.844775"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in CPI adjustment data csv\n",
    "cpi_df = pd.read_csv('MRTS/CPI_Index_calc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge inflation adjustment data and mrts data\n",
    "full_df = pd.merge(mrts_df, cpi_df[['DATE', 'feb2021_multiple']], left_on='month_start', right_on='DATE')\n",
    "\n",
    "# add inflation adjustment column.\n",
    "full_df['2021_infl_seas_adjd'] = round(full_df['Raw_Amount'] * full_df['feb2021_multiple'])\n",
    "\n",
    "# creating table in its final form\n",
    "final_df = full_df[['NAICS_code', 'Business_type', 'month_year', 'year', 'month', 'month_start', 'Raw_Amount', '2021_infl_seas_adjd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing new csv file for import to MySQL\n",
    "csv_file = f\"MRTS/MRTS_year_files/MRTS_Data.csv\"\n",
    "final_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.5 Writing an Installation Script\n",
    "\n",
    "Describe how you wrote a Python installation script to read your dataset in MySQL WorkBench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cnx = mysql.connector.connect(\n",
    "    user = 'root',\n",
    "    password = '7890oiuy&*()OIUY',\n",
    "    host = '127.0.0.1',\n",
    "    #   database = 'sakila',\n",
    "    auth_plugin = 'mysql_native_password'\n",
    ")\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "query = (\"select 'this is open'\")\n",
    "cursor.execute(query)\n",
    "\n",
    "# for row in cursor.fetchall():\n",
    "#     print(row)\n",
    "\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Back to top](#Index)\n",
    "\n",
    "## 3. Analysis and Visualization\n",
    "\n",
    "For each of the sections below, make sure you include a description of the steps you followed. Whenever possible, include screenshots of your code or program windows to demonstrate your steps.\n",
    "\n",
    "Here, describe the differences, advantages, and disadvantages of running *queries* against your dataset using the MySQL Workbench or a Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.1 Running Queries in MySQL Workbech\n",
    "\n",
    "Describe which *queries* you ran against the MRTS dataset in MySQL Workbench to verify that everything worked as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.2 Running Queries From Python\n",
    "\n",
    "Describe how you tested the previous *queries* on the the MRTS dataset using a Python script and the Terminal window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.3 Explore Trends\n",
    "\n",
    "Describe which *queries* you wrote the explore the differences in trends between various categories in your data.\n",
    "\n",
    "In your submission make sure to answer the following:\n",
    "\n",
    "- What is an economic trend and why is it considered an important measure to predict quantities, like spending patterns?\n",
    "- What is the trend of the retail and food services categories? Can this data be displayed clearly or do you need to adjust some parameters to reduce extraneous details and be able to visualize a clean trend?\n",
    "- When comparing businesses like bookstores, sporting goods stores, and hobbies, toys, and games stores, what is the highest trend of all of these options? Which one grew faster? Which one is higher? Is there a seasonal pattern? Were there any changes in 2020? Which is better, monthly or yearly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.4 Explore Percentage Change\n",
    "\n",
    "Describe which *queries* you wrote to explore the differences in trends between various categories in your data.\n",
    "\n",
    "In your submission make sure to answer the following:\n",
    "\n",
    "- In economics, what is the percentage change and why is it considered an important measure to predict quantities like spending patterns?\n",
    "- Consider the women's clothing and men's clothing businesses and their percentage change. How are these two businesses related? For each of the two businesses, what is the percentage of contribution to the whole and how does it change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.5 Explore Rolling Time Windows\n",
    "\n",
    "\n",
    "Describe which *queries* you wrote to explore the differences in trends between various categories in your data.\n",
    "\n",
    "In your submission, make sure to answer the following:\n",
    "\n",
    "- In economics, what is the rolling time window and why is it considered an important measure to predict quantities like spending patterns?\n",
    "- Consider at least two businesses of your own from the MRTS data. Which *queries* did you write to analyze and produce graphs of rolling time windows for the chosen categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Describe your conclusions. Which one of the businesses considered seems like it's going to attract the least spending? Which business seems likely to attract the most spending? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Back to top](#Index\n",
    ")\n",
    "## References\n",
    "\n",
    "Add all references you used to complete this project.\n",
    "\n",
    "Use this format for articles:\n",
    "- Author Last Name, Author First Name. “Article Title.” Journal Title Volume #, no. Issue # (year): page range.\n",
    "\n",
    "- Ex: Doe, John. “Data Engineering.” Data Engineering Journal 18, no. 4 (2021): 12-18.\n",
    "\n",
    "Use this format for websites:\n",
    "- Author Last Name, Author First Name. “Title of Web Page.” Name of Website. Publishing organization, publication or revision date if available. Access date if no other date is available. URL .\n",
    "\n",
    "- Doe, John. “Data Engineering.” Data Engineer Resource. Cengage, 2021. www.dataengineerresource.com .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
